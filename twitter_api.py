# -*- coding: utf-8 -*-
"""Twitter_API.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J0QT9IxTFeCJSrTQkLvqhb0edA4bpQaL

# Twitter authentication
ask professor how to get the results only from twitter.com
"""

import requests
from requests_oauthlib import OAuth1

import pandas as pd
import json

# consumer and access keys and secrets for getting access to Twitter API through your application
consumer_key = 'clxkjAjyULSovQFBxqtnK8FZM'
consumer_secret = 'KDhq8VUgXo58EHKphp3KRmhLdAADRd6LqCm99T4oQ80Q2dICvD'
access_token = '1624630802876080129-Fd8A5YRnxV1hcMSw675A9RxTHPIxOo'
access_secret = 'b3NLl9J2YFjz9nkSLod08NHlomjTC7s6rVRe8tSsxOTa2'

auth = OAuth1(consumer_key, consumer_secret, access_token, access_secret)

"""# Building a query"""

def create_url(keyword):
    
    url = 'https://api.twitter.com/2/search/tweets.json'

    query_params = {'q': keyword,
                    'count' : 50, 
                    'lang' : "en"   
                    }

    res = requests.get(url, params=query_params, auth=auth)
    return (res.json())

df = create_url("AAPL")

df

"""# Creating dataframe with information about the tweets that inclucde names of the stocks from the list"""

pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def df_from_api(company_list):
  df = pd.DataFrame(columns = ['post', 'keyword', 'date', 'link', "sentiment_score"])

  # to make sure that the tweets we see are connected with APPL as a stock 
  for i in range(len(company_list)):
    company_list[i] = company_list[i] + " AND " + "stock"

  # compund sentiment_score per stock
  compund_sentiment_per_stock = {}


  for i in company_list:
    tweets = create_url(i)
    for tweet in tweets['statuses']:
      if len(tweet["entities"]["user_mentions"])!=0:
        if tweet["text"] in df["post"].values:
          continue
        else:
          stock = i.split(" ")[0]
          sentiment_dict = SentimentIntensityAnalyzer().polarity_scores(tweet["text"])
          url = "https://twitter.com/i/web/status/" + tweet["id_str"]
          row = [tweet["text"], stock, tweet["created_at"], url, sentiment_dict["compound"]]
          df.loc[len(df.index)] = row

    stock_df = df.loc[df['keyword'] == stock]
    compund_sentiment_per_stock[stock] = round(stock_df["sentiment_score"].mean(), 3)
  print(compund_sentiment_per_stock)
  
  return df

data = df_from_api(["APPL","TSLA"])

import requests
import json

# To set your environment variables in your terminal run the following line:
# export 'BEARER_TOKEN'='<your_bearer_token>'
bearer_token = "AAAAAAAAAAAAAAAAAAAAAHBonAEAAAAAbqwTRFpTcRqZ2DkJhDW05KA7TZA%3DH4hmHoU0W6JXNePFs0R0U74vcq1pRTT53jrCgwDpfKMWV4qGEJ"

search_url = "https://api.twitter.com/2/spaces/search"

search_term = 'AAPL' # Replace this value with your search term

# Optional params: host_ids,conversation_controls,created_at,creator_id,id,invited_user_ids,is_ticketed,lang,media_key,participants,scheduled_start,speaker_ids,started_at,state,title,updated_at
query_params = {'query': search_term, 'space.fields': 'title,created_at', 'expansions': 'creator_id'}


def create_headers(bearer_token):
    headers = {
        "Authorization": "Bearer {}".format(bearer_token),
        "User-Agent": "v2SpacesSearchPython"
    }
    return headers


def connect_to_endpoint(url, headers, params):
    response = requests.request("GET", search_url, headers=headers, params=params)
    print(response.status_code)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()


def main():
    headers = create_headers(bearer_token)
    json_response = connect_to_endpoint(search_url, headers, query_params)
    print(json.dumps(json_response, indent=4, sort_keys=True))

main()